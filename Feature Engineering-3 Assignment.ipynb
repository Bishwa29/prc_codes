{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8ebde-344d-49a8-b536-2cbbd47f7f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Feature Engineering-3 Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee146fb-24e4-4e59-96b5-d5c38affe727",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "##A1. Min-max scaling is a normalization technique in feature scaling where the data is converted into a range of 0 to 1 in order to make the data agnostic of its units.\n",
    "\n",
    "##This is particularly useful when working with algorithms that require features to be on a similar scale, such as neural networks, SVMs, and algorithms that use distance metrics like K-Nearest Neighbors.\n",
    "\n",
    "##E.g-Suppose we have a dataset of ages=[10,20,30] and weights=[50,60,70]. Now these two variables have different units of measurement and hence, different scales.\n",
    "##We can use the min-max scalar to normalize these datasets\n",
    "\n",
    "##For age, applying the formula, xscaled=(x-xmin)/(xmax-xmin)\n",
    "## scaled age dataset=[0,0.5,1] and weights dataset=[0,0.5,1]\n",
    "##Now these scaled datapoints can be used together to run any model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ff329-fc67-424e-9391-254549dbd3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "##A2. The Unit Vector technique in feature scaling, also known as Normalization, scales each data point such that the feature vector has a magnitude of 1. \n",
    "##This technique is particularly useful when the magnitude of the features is important but you still want to normalize them. \n",
    "\n",
    "##How it differs from Min-Max Scaling:\n",
    "##Range: Min-Max scaling scales the data to a fixed range (e.g., [0, 1]), whereas Unit Vector normalization scales each data point to have a magnitude of 1.\n",
    "##Magnitude: Min-Max scaling preserves the original range of the data, whereas Unit Vector normalization adjusts the magnitude of the feature vectors.\n",
    "##Direction: Unit Vector normalization adjusts the direction of the feature vectors, aligning them such that they all have a magnitude of 1.\n",
    "\n",
    "##Example:\n",
    "##Suppose you have a dataset with two features representing heights (in inches) and weights (in pounds):\n",
    "\n",
    "##70 & 160\n",
    "##65 & 150\n",
    "##80 & 180\n",
    "\n",
    "##Normalize Each Data Point:\n",
    "##Normalize each data point by dividing it by its Euclidean norm:\n",
    "\n",
    "##ùëãnormalized=ùëã/‚à•ùëã‚à•\n",
    "##After normalization, the dataset might look like this (approximate values):\n",
    "\n",
    "##data_normalized=[0.401  0.917  0.387  0.922  0.427  0.904]\n",
    "\n",
    "##In this transformed dataset, each row (data point) now has a magnitude of 1. This technique is useful in scenarios where the direction of the feature vector relative to others is important, rather than the absolute magnitude. It ensures that all feature vectors are treated equally in terms of their orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db21f87-591b-4246-baaa-425eddd48794",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "##A3. PCA (Principal Component Analysis) is a statistical technique used for reducing the dimensionality of data while preserving its important underlying structure. \n",
    "##It achieves this by transforming the original variables into a new set of orthogonal variables called principal components (PCs). \n",
    "##These principal components are ordered by the amount of variance they explain in the data, with the first PC capturing the maximum variance present in the data, the second PC capturing the second most variance, and so on.\n",
    "\n",
    "##Example:\n",
    "##Consider a dataset with two features: height (in inches) and weight (in pounds) of individuals. We want to reduce this dataset from two dimensions to one dimension using PCA.\n",
    "\n",
    "##Apply PCA:\n",
    "\n",
    "##Standardize the Data: Subtract the mean and divide by the standard deviation for each feature.\n",
    "##Compute Covariance Matrix: Compute the covariance matrix of the standardized data.\n",
    "##Eigenvalue Decomposition: Perform eigenvalue decomposition to obtain eigenvectors and eigenvalues.\n",
    "##Select Principal Components: Since we want to reduce to 1 dimension, we select the first principal component (the one with the highest eigenvalue)\n",
    "##Transform the original data into the new one-dimensional space using the selected principal component.\n",
    "\n",
    "##PCA is beneficial in reducing the dimensionality of data while retaining most of the information present in the original dataset. \n",
    "##It is widely used in various fields such as image processing, bioinformatics, finance, and more, where high-dimensional data needs to be analyzed efficiently or visualized effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0358030-ef62-4fc9-9f4b-84f19f86bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "##A4. PCA (Principal Component Analysis) can be closely related to feature extraction in the context of reducing the dimensionality of data by transforming original features into a smaller set of derived features (principal components) that capture the essential information present in the data.\n",
    "##Using PCA for Feature Extraction - Example:\n",
    "##Consider a dataset containing information about students' performance in different subjects: Math, Science, and English. \n",
    "##Each student is described by these three scores. We want to extract features that capture the overall academic performance while reducing the dimensionality of the dataset.\n",
    "\n",
    "##Suppose we have the following dataset (scores out of 100):\n",
    "##data=[85  90  80  70  85  75  95  70  85  80  75  90]\n",
    "\n",
    "##Apply PCA:\n",
    "\n",
    "##Standardize the Data: Standardize the scores (subtract mean, divide by standard deviation).\n",
    "##Compute Covariance Matrix: Compute the covariance matrix of the standardized data.\n",
    "##Eigenvalue Decomposition: Perform eigenvalue decomposition to obtain eigenvectors and eigenvalues.\n",
    "##Select Principal Components: Determine how many principal components to retain based on the explained variance ratio or by specifying the number of components.\n",
    "##Transform the Data: Project the original data onto the selected principal components to obtain the transformed (extracted) features.\n",
    "##Suppose after PCA, we decide to retain the first principal component (which explains the most variance):\n",
    "\n",
    "##The first principal component might be a linear combination of the original features, such as PC1=0.6‚ãÖMath + 0.7‚ãÖScience + 0.5‚ãÖEnglish\n",
    "\n",
    "##In this example, PCA has been used as a feature extraction technique to derive a new feature (principal component) that represents the overall academic performance of each student, effectively reducing the dimensionality of the dataset from three features (scores) to one feature (principal component). \n",
    "##This extracted feature can now be used for further analysis or modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b385e1-790d-47ac-8725-4b95d36ab629",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "##A5. The min max scaling would be used on each of the three features: price, rating, delivery time to normalize them and bring them all to a scale of [0,1].\n",
    "##Pre-reprocessing the data using Min-Max scaling can help ensure that the features such as price, rating, and delivery time are on a similar scale. \n",
    "##This is important because recommendation algorithms often rely on computing distances or similarities between data points, and having features on different scales could skew the results.\n",
    "\n",
    "##By preprocessing the data with Min-Max scaling, you prepare it for use in various recommendation algorithms (such as collaborative filtering or content-based filtering) where normalized features can lead to more accurate and balanced recommendations for users of the food delivery service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14e1dc-f758-45be-aec3-533ba52ba60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "##A6. In the context of building a model to predict stock prices using a dataset with numerous features (such as company financial data and market trends), PCA (Principal Component Analysis) can be used effectively to reduce the dimensionality of the dataset. This reduction helps in simplifying the model, improving computational efficiency, and potentially enhancing model performance by focusing on the most significant features or patterns in the data.\n",
    "\n",
    "##Steps to Use PCA for Dimensionality Reduction:\n",
    "##Dataset Understanding:\n",
    "##Start by understanding the dataset, which includes features related to company financial metrics (e.g., revenue, earnings, debt) and market trends (e.g., sector performance, interest rates, GDP growth).\n",
    "##Data Preprocessing:\n",
    "##Standardize the data: It's crucial to standardize or normalize the features so that they have a mean of 0 and a standard deviation of 1. This ensures that all features contribute equally to the PCA analysis.\n",
    "##Apply PCA:\n",
    "##Compute the covariance matrix: Calculate the covariance matrix of the standardized dataset. The covariance matrix shows the relationships between pairs of variables.\n",
    "##Perform eigen decomposition: This involves calculating the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions (principal components) of maximum variance in the data, and eigenvalues indicate the magnitude of variance along each eigenvector.\n",
    "##Select Principal Components:\n",
    "##Determine the number of principal components (PCs) to retain: Choose the number of principal components based on the explained variance ratio or a desired number of dimensions. PCA orders the principal components such that the first PC explains the maximum variance, the second PC explains the second most variance, and so on.\n",
    "##Transform the Data:\n",
    "##Project the original data onto the selected principal components: This transformation yields a reduced-dimensional representation of the dataset where each data point is represented by the values along the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f0ab63-ad1c-49b7-928a-2ac507aa86ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [1, 5, 10, 15, 20]\n",
      "Scaled data (-1 to 1): [-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "##Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "##A7.\n",
    "def min_max_scaling(data, min_val=-1, max_val=1):\n",
    "    # Calculate min and max of the dataset\n",
    "    data_min = min(data)\n",
    "    data_max = max(data)\n",
    "    \n",
    "    # Apply Min-Max scaling\n",
    "    scaled_data = [((x - data_min) / (data_max - data_min)) * (max_val - min_val) + min_val for x in data]\n",
    "    \n",
    "    return scaled_data\n",
    "\n",
    "# Example usage with the dataset [1, 5, 10, 15, 20]\n",
    "data = [1, 5, 10, 15, 20]\n",
    "scaled_data = min_max_scaling(data, -1, 1)\n",
    "\n",
    "print(\"Original data:\", data)\n",
    "print(\"Scaled data (-1 to 1):\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a903e-40d0-42e4-aff3-7f58c5b36c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "##A8.To perform feature extraction using PCA (Principal Component Analysis) on a dataset containing features like height, weight, age, gender, and blood pressure, the choice of how many principal components to retain depends on several factors related to the dataset's characteristics and the goals of the analysis.\n",
    "\n",
    "##PCA would be performed using the standard approach:\n",
    "##Standardize the Data: Standardize the data (subtract mean, divide by standard deviation).\n",
    "##Compute Covariance Matrix: Compute the covariance matrix of the standardized data.\n",
    "##Eigenvalue Decomposition: Perform eigenvalue decomposition to obtain eigenvectors and eigenvalues.\n",
    "##Select Principal Components: Determine how many principal components to retain based on the explained variance ratio or by specifying the number of components.\n",
    "##Select Number of Principal Components:\n",
    "##Determine how many principal components to retain based on the explained variance ratio or a desired cumulative explained variance.\n",
    "##Typically, you can plot the cumulative explained variance versus the number of principal components and choose a number that captures a significant portion of the total variance (e.g., 95% or more).\n",
    "\n",
    "##Considerations for Choosing the Number of Principal Components:\n",
    "\n",
    "##Explained Variance: Check the cumulative explained variance ratio. You want to retain enough principal components to capture a high percentage (e.g., 95% or more) of the total variance in the dataset.\n",
    "\n",
    "##Dimensionality Reduction: PCA is used to reduce the dimensionality of the dataset while retaining as much variance as possible. Choosing fewer principal components leads to a more compact representation of the data but may lose some information.\n",
    "\n",
    "##Application Requirements: Consider the specific requirements of your application or analysis. If interpretability is important, you might choose to retain fewer principal components that are easier to interpret."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
