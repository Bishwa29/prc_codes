{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7923cfcc-bc98-4c5e-aa38-f7283580e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Regression-1 Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6213603-532e-4fa4-b457-49154b6c1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "##A1. Simple linear regression is used to determine a linear or straight line relationship between one independent feature and one output variable.\n",
    "##On the other hand, multiple linear regression determines a linear relationship between multiple independent features and one output variable.\n",
    "##Instead of a single predictor, we now have multiple predictors, each with its own coefficient. The relationship is still assumed to be linear, and the model estimates the effect of each independent variable on the dependent variable while holding the other variables constant.\n",
    "##Example of simple linear regression is determining a linear relationship between number of hours studied and marks scored in a subject in an exam.\n",
    "##Example of multiple linear regression is determining the price of property considering multiple independent variables such as size of property, kind of locality, etc.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e0d6d-d88e-4cb3-96bd-73f47024ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "##A2. The assumptions of linear regression are:\n",
    "##The independent and dependent variable should be numerical in nature\n",
    "##The change in the outcome variable is proportional to the change in the predictor variables\n",
    "##The errors (residuals) generated by the model are independent of each other. This assumption ensures that there is no autocorrelation between consecutive errors.\n",
    "##In multiple linear regression, the independent variables should not be perfectly correlated with each other. This means there should not be a linear relationship among the independent variables.\n",
    "\n",
    "##To determine whether these assumptions hold in a given dataset, several diagnostic techniques can be employed:\n",
    "##Residual Analysis: Examine the residuals (observed values minus predicted values) to assess linearity, independence of errors, and homoscedasticity.\n",
    "##Normality Check: \n",
    "    ##Normal Q-Q Plot: Plot the standardized residuals against the theoretical quantiles of a normal distribution. A roughly straight line indicates normality.\n",
    "##Multicollinearity Check (for Multiple Linear Regression):\n",
    "    ##Variance Inflation Factor (VIF): Calculate VIF for each predictor variable to assess the extent of multicollinearity. VIF values above 10 are often considered problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf22af-1ca7-449f-b784-60b7db81bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "##The slope and intercept are two key integral parts of a linear regression equation or a straight line equation.\n",
    "##The slope represents change in the dependent variable based on unit change in independent variable. In other words, this determines the magnitude and direction of linearity of dependent with independent variable.\n",
    "##Intercept refers to value of dependent variable when the independent variable is zero.\n",
    "\n",
    "##Example Scenario:\n",
    "\n",
    "##Suppose we want to predict the sales (Y) of a product based on the amount spent on advertising (X). We collect data from previous campaigns where we know the amount spent on advertising and the corresponding sales figures.\n",
    "##Interpretation of Slope (ùõΩ1): Let's say the slope (ùõΩ1) of our linear regression model is 0.5.\n",
    "##This means that for every additional unit of currency spent on advertising, we expect sales to increase by 0.5 units (e.g., 0.5 units could be in thousands of dollars, depending on the scale of the variables).\n",
    "##Interpretation of Intercept (ùõΩ0):Suppose the intercept (ùõΩ0) of our model is 10.\n",
    "##This means that if we were to spend zero currency units on advertising (hypothetically), we would still expect sales to be 10 units (e.g., 10 units of products sold, whatever the product unit may be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f498fd-cfb3-471e-87d8-6f4067be66d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "##A4. Gradient descent is a fundamental optimization algorithm used in machine learning and deep learning to minimize the loss function or cost function associated with a model. \n",
    "##It is an iterative algorithm that adjusts the parameters of the model in order to minimize a given function.\n",
    "\n",
    "##The main goal of gradient descent is to find the set of parameters (weights) for a model that minimizes a cost function J(Œ∏). This cost function measures how well the model's predictions match the actual values (labels) in the training data.\n",
    "\n",
    "##Gradient descent is used extensively in machine learning for training models, especially in cases where:\n",
    "\n",
    "##The model parameters cannot be calculated directly through closed-form solutions due to the complexity or size of the dataset.\n",
    "##The number of parameters is large, making iterative optimization more feasible than other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2266c5e-9586-4a09-a897-83407b249b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "##A5. Multiple linear regression determines a linear relationship between multiple independent features and one output variable.\n",
    "##Instead of a single predictor, we now have multiple predictors, each with its own coefficient. \n",
    "##The relationship is still assumed to be linear, and the model estimates the effect of each independent variable on the dependent variable while holding the other variables constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c44de3-5418-4e16-8b21-a1b85faca840",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "##A6. Multicollinearity in the context of multiple linear regression occurs when there are high correlations among predictor variables. \n",
    "##This correlation can cause issues in the regression analysis because it undermines the statistical significance of individual predictors.\n",
    "\n",
    "##Detection of Multicollinearity:\n",
    "##Correlation Matrix: Calculate the correlation matrix among all predictor variables. High correlations (typically above 0.7 or 0.8) indicate potential multicollinearity.\n",
    "##Variance Inflation Factor (VIF):VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity.\n",
    "##Calculate VIF for each predictor variable. A VIF greater than 10 (some use a threshold of 5) indicates multicollinearity.\n",
    "\n",
    "##Addressing Multicollinearity:\n",
    "##Feature Selection: Remove one or more of the highly correlated variables. Choose the variables that are most relevant to the outcome and drop the others.\n",
    "##Dimensionality Reduction: Use techniques like Principal Component Analysis (PCA) to transform the predictors into a smaller set of uncorrelated components.\n",
    "##PCA reduces multicollinearity by creating new orthogonal variables (principal components) that are linear combinations of the original predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e65ff-688d-4cfc-ae8a-7b417aca8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "##A7. Polynomial regression model is one which considers the non-linear relationship between the output variable and one or more independent features.\n",
    "##This model is used to understand the non-linear relationship between independent and dependent variables.\n",
    "##It is different from linear regression in the sense that linear regression assumes a linear or straight line relationship between variables.\n",
    "##This model assumes a non-linear (parabolic, hyperbolic) relationship between independent and dependent features.\n",
    "\n",
    "##Polynomial regression is a form of regression analysis where the relationship between the independent variable ùë• and the dependent variable ùë¶ is modeled as an ùëõ-th degree polynomial in x. \n",
    "##This allows the model to capture nonlinear relationships between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f87a68-c79f-417a-8b78-ad98a7c91116",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? \n",
    "##In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "##A8. Advantages of Polynomial Regression:\n",
    "\n",
    "##Flexibility in Modeling Non-linear Relationships:\n",
    "##Polynomial regression can capture nonlinear relationships between the independent and dependent variables more accurately than linear regression.\n",
    "##It allows for curves and bends in the data, providing a better fit for datasets where relationships are not strictly linear.\n",
    "\n",
    "##Higher Order Trends:\n",
    "##It can model relationships where the rate of change of the dependent variable with respect to the independent variable varies across the range of the independent variable.\n",
    "##This flexibility allows for capturing more complex patterns in the data.\n",
    "\n",
    "##No Need for Transformations:\n",
    "##Unlike in linear regression where transforming variables (e.g., log transformation) may be needed to fit nonlinear relationships, polynomial regression directly models such relationships.\n",
    "\n",
    "##Disadvantages of Polynomial Regression:\n",
    "\n",
    "##Overfitting:\n",
    "##Polynomial regression with high degrees (e.g., degree 3 or more) can lead to overfitting, where the model captures noise or random fluctuations in the data rather than the underlying relationship.\n",
    "##Overfitted models perform well on training data but generalize poorly to new, unseen data.\n",
    "\n",
    "##Increased Complexity:\n",
    "##As the degree of the polynomial increases, the model becomes more complex with more parameters to estimate.\n",
    "##Interpreting the coefficients of higher-degree polynomials becomes more challenging and less intuitive.\n",
    "\n",
    "##Data Requirements:\n",
    "##Polynomial regression requires careful consideration of the degree of the polynomial. \n",
    "##Choosing an excessively high degree without sufficient data can lead to unreliable estimates.\n",
    "\n",
    "##When to Prefer Polynomial Regression:\n",
    "\n",
    "##Nonlinear Relationships:\n",
    "##Use polynomial regression when there is a prior belief or evidence of a nonlinear relationship between the independent and dependent variables.\n",
    "##It is suitable for datasets where the relationship does not follow a straight line but shows curves, bends, or other nonlinear patterns.\n",
    "\n",
    "##Exploratory Analysis:\n",
    "##In exploratory data analysis, polynomial regression can be used to uncover and visualize complex relationships between variables.\n",
    "##It can provide insights into how the relationship changes across different ranges of the independent variable.\n",
    "\n",
    "##Small to Moderate Degree Polynomials:\n",
    "##Prefer lower-degree polynomials (e.g., quadratic or cubic) to balance model complexity and model fit.\n",
    "##Lower-degree polynomials are less prone to overfitting and can still capture significant nonlinear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da3f57-d5b4-499b-b981-6e06005c2cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a7368-17af-45c6-85d3-5a6c087a12de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50227a5-39a2-4a14-995f-3ccfb3a26609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46590ed-304d-4688-bc0c-83705f4352d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6deef3-f40e-4266-aadd-d9abe0250abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
